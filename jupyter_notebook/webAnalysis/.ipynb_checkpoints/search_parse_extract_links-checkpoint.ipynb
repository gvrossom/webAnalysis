{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Trying target domain: cer.be\n",
      "[*] Trying index 2016-40\n",
      "[*] Added 9 results.\n",
      "[*] Found a total of 9 hits.\n",
      "[*] Trying target domain: eimrail.org\n",
      "[*] Trying index 2016-40\n",
      "[*] Added 3 results.\n",
      "[*] Found a total of 3 hits.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cer.be': 9, 'eimrail.org': 3}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "import StringIO\n",
    "import gzip\n",
    "import csv\n",
    "import codecs\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "domain_list = [\"cer.be\", \"eimrail.org\"]\n",
    "index_list = [\"2016-40\"]\n",
    "\n",
    "#\n",
    "# Searches the Common Crawl Index for a domain.\n",
    "#\n",
    "def search_domain(domain):\n",
    "\n",
    "    record_list = []\n",
    "    \n",
    "    print \"[*] Trying target domain: %s\" % domain\n",
    "    \n",
    "    for index in index_list:\n",
    "        \n",
    "        print \"[*] Trying index %s\" % index\n",
    "        \n",
    "        cc_url  = \"http://index.commoncrawl.org/CC-MAIN-%s-index?\" % index\n",
    "        cc_url += \"url=%s&matchType=domain&output=json\" % domain\n",
    "        \n",
    "        response = requests.get(cc_url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            \n",
    "            records = response.content.splitlines()\n",
    "            \n",
    "            for record in records:\n",
    "                record_list.append(json.loads(record))\n",
    "            \n",
    "            print \"[*] Added %d results.\" % len(records)\n",
    "            \n",
    "    \n",
    "    print \"[*] Found a total of %d hits.\" % len(record_list)\n",
    "    \n",
    "    return record_list        \n",
    "\n",
    "#\n",
    "# Downloads a page from Common Crawl - adapted graciously from @Smerity - thanks man!\n",
    "# https://gist.github.com/Smerity/56bc6f21a8adec920ebf\n",
    "#\n",
    "def download_page(record):\n",
    "\n",
    "    offset, length = int(record['offset']), int(record['length'])\n",
    "    offset_end = offset + length - 1\n",
    "\n",
    "    # We'll get the file via HTTPS so we don't need to worry about S3 credentials\n",
    "    # Getting the file on S3 is equivalent however - you can request a Range\n",
    "    prefix = 'https://commoncrawl.s3.amazonaws.com/'\n",
    "    \n",
    "    # We can then use the Range header to ask for just this set of bytes\n",
    "    resp = requests.get(prefix + record['filename'], headers={'Range': 'bytes={}-{}'.format(offset, offset_end)})\n",
    "    \n",
    "    # The page is stored compressed (gzip) to save space\n",
    "    # We can extract it using the GZIP library\n",
    "    raw_data = StringIO.StringIO(resp.content)\n",
    "    f = gzip.GzipFile(fileobj=raw_data)\n",
    "    \n",
    "    # What we have now is just the WARC response, formatted:\n",
    "    data = f.read()\n",
    "    \n",
    "    response = \"\"\n",
    "    \n",
    "    if len(data):\n",
    "        try:\n",
    "            warc, header, response = data.strip().split('\\r\\n\\r\\n', 2)\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    return response\n",
    "\n",
    "\n",
    "#\n",
    "# Extract links from the HTML  \n",
    "#\n",
    "def extract_external_links(html_content,link_list, domain):\n",
    "\n",
    "    parser = BeautifulSoup(html_content, \"html.parser\")\n",
    "        \n",
    "    links = parser.find_all(\"a\")\n",
    "    \n",
    "    if links:\n",
    "        \n",
    "        for link in links:\n",
    "            href = link.attrs.get(\"href\")\n",
    "            \n",
    "            if href is not None:\n",
    "                \n",
    "                if domain not in href:\n",
    "                    if href not in link_list and href.startswith(\"http\"):\n",
    "                        print \"[*] Discovered external link: %s\" % href\n",
    "                        link_list.append(href)\n",
    "\n",
    "    return link_list\n",
    "\n",
    "\n",
    "def extract_domain_name(link_list):\n",
    "    extracted_domain_list = []\n",
    "    for link in link_list:\n",
    "        website = link.split(\"/\")[2]\n",
    "        if \"ww\" in website:\n",
    "            website = website[website.find('.')+1:]\n",
    "        extracted_domain_list.append(website)\n",
    "    \n",
    "    return extracted_domain_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_data(domain_list):\n",
    "    master_dict = {}\n",
    "    \n",
    "    for domain in domain_list:\n",
    "\n",
    "        record_list = search_domain(domain)\n",
    "        link_list   = []\n",
    "\n",
    "        for record in record_list:\n",
    "\n",
    "            html_content = download_page(record)\n",
    "\n",
    "            # print \"[*] Retrieved %d bytes for %s\" % (len(html_content),record['url'])\n",
    "\n",
    "            link_list = extract_external_links(html_content,link_list,domain)\n",
    "\n",
    "            master_dict[domain] = extract_domain_name(link_list)\n",
    "\n",
    "\n",
    "        # print \"[*] Total external links discovered: %d\" % len(link_list)\n",
    "    \n",
    "    return master_dict\n",
    "\n",
    "search_results = {}\n",
    "\n",
    "for domain in domain_list:\n",
    "    data = search_domain(domain)\n",
    "    search_results[domain] = len(data)\n",
    "    \n",
    "search_results\n",
    "# with codecs.open(\"%s-links.csv\" % domain,\"wb\",encoding=\"utf-8\") as output:\n",
    "\n",
    "#    fields = [\"URL\"]\n",
    "    \n",
    "#    logger = csv.DictWriter(output,fieldnames=fields)\n",
    "#    logger.writeheader()\n",
    "    \n",
    "#    for link in link_list:\n",
    "#        logger.writerow({\"URL\":link})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eimrail.org\n",
      "cer.be\n",
      "the full list: ['eimrail.org', 'cer.be']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<networkx.classes.graph.Graph at 0xaf3e70ec>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "G.add_nodes_from(domain_list)\n",
    "\n",
    "for node in G.nodes():\n",
    "    print node\n",
    "    \n",
    "print \"the full list:\", G.nodes()\n",
    "\n",
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the full dict: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 'eimrail.org', 'cer.be']\n"
     ]
    }
   ],
   "source": [
    "H = nx.path_graph(10)\n",
    "G.add_nodes_from(H)\n",
    "\n",
    "print \"the full dict:\", G.nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the full dict: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 'eimrail.org', 'cer.be', <networkx.classes.graph.Graph object at 0xaf066f8c>]\n"
     ]
    }
   ],
   "source": [
    "G.add_node(H)\n",
    "print \"the full dict:\", G.nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 2), (2, 3)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(u'http://www.apotheka.gr/img/relogiosreplica.php', 'eimrail.org'),\n",
       " (u'https://eim.viadesk.com/do/userlogin', 'eimrail.org'),\n",
       " (u'https://www.facebook.com/pages/European-Rail-Infrastructure-Managers-EIM/130752031858',\n",
       "  'eimrail.org'),\n",
       " (u'http://www.eagleproducts.com.au/melhor.php', 'eimrail.org'),\n",
       " (u'http://pabirds.org/Search/relogios.php', 'eimrail.org'),\n",
       " (u'http://www.vellendtech.com/homens.php', 'eimrail.org'),\n",
       " (u'http://ezineturk.com/indice.asp', 'eimrail.org'),\n",
       " (u'http://vlbk.se/home.asp', 'eimrail.org'),\n",
       " ('eimrail.org', u'http://onlyoffice.com/fr/?campaign=nonprofit'),\n",
       " ('eimrail.org', u'http://barsuraube.org/homepage.php'),\n",
       " ('eimrail.org', u'https://www.linkedin.com/company/1239529'),\n",
       " ('eimrail.org', u'http://www.jac.eu/products.asp')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "G.add_edge(1,2)\n",
    "e = (2,3)\n",
    "G.add_edge(*e)\n",
    "\n",
    "DoubleG = nx.Graph()\n",
    "for link in master_dict['eimrail.org']:\n",
    "    DoubleG.add_edge('eimrail.org', link)\n",
    "    \n",
    "print G.edges()\n",
    "DoubleG.edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nx.draw(DoubleG)\n",
    "plt.savefig(\"test.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
